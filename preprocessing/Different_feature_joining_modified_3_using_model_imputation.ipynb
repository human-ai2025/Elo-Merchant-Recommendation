{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Different_feature_joining_modified_3_using_model_imputation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/human-ai2025/Elo-Merchant-Recommendation/blob/master/Feature%20Enginering/Different_feature_joining_modified_3_using_model_imputation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oeG4uYVxOap"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSNUHQTd_jbt"
      },
      "source": [
        "#Import Libraries \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "import gc\r\n",
        "from contextlib import contextmanager\r\n",
        "from pandas.core.common import SettingWithCopyWarning\r\n",
        "import datetime\r\n",
        "import time\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "from scipy.stats import mode\r\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTKdW0Kx8ku"
      },
      "source": [
        "### Mount the drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0gy8in1_jYK",
        "outputId": "0b9d7c38-6c50-4b57-eb3c-3a39f9f38242"
      },
      "source": [
        "#Mounting drive \r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjAQaUCAx-7P"
      },
      "source": [
        "### Setup the current path "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG9WyuKGyWx-",
        "outputId": "f8d63b8f-f9df-4c88-9c3d-2bab11722910"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feJTkhbC_jVa",
        "outputId": "bb2d994c-aad7-43c5-8fc0-32f9e36bf3b6"
      },
      "source": [
        "#Setting up worksapce directory \r\n",
        "%cd /content/drive/MyDrive/data "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQIFTxv1ydQ7",
        "outputId": "2bd2077d-0402-4cc4-bc17-3931e984407f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " augmented_test.csv\n",
            " augmented_test_try1.csv\n",
            " augmented_train.csv\n",
            " augmented_train_try1.csv\n",
            " clf_cat2.sav\n",
            " clf_cat3.sav\n",
            "'Data Dictionary.xlsx'\n",
            " Data_Dictionary.xlsx\n",
            " historical_transactions.csv\n",
            " historical_transactions_df.pkl\n",
            " merchant_category_1_enc.npy\n",
            " merchant_category_4_enc.npy\n",
            " merchant_most_recent_purchases_range_enc.npy\n",
            " merchant_most_recent_sales_range_enc.npy\n",
            " merchants.csv\n",
            " merged_transactions_with_merchants.csv\n",
            " new_merchant_authorized_flag_enc.npy\n",
            " new_merchant_category_1_enc.npy\n",
            " new_merchant_df.pkl\n",
            " new_merchant_transactions.csv\n",
            " sample_submission.csv\n",
            " test.csv\n",
            " train.csv\n",
            " transactions_raw_merged.csv\n",
            " transactions_refined_1_merged_WON.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXBfyhiOxWKf"
      },
      "source": [
        "## Timer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxDmUU0b_jSw"
      },
      "source": [
        "#refer:-https://www.youtube.com/watch?v=vOMtQ4ocMGI\r\n",
        "@contextmanager\r\n",
        "def timer(title):\r\n",
        "    \"\"\" used to calculate time for each function\"\"\"\r\n",
        "    t0 = time.time()\r\n",
        "    yield\r\n",
        "    print(\"{} - done in {:.000f}s\".format(title, time.time() - t0))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjkNfzYyxX_3"
      },
      "source": [
        "## Memory Reduction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiFE8Io2_jQM"
      },
      "source": [
        "#https://www.kaggle.com/fabiendaniel/elo-world\r\n",
        "#Function to load data into pandas and reduce memory usage\r\n",
        "\r\n",
        "def reduce_mem_usage(df, verbose=True):\r\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\r\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \r\n",
        "    #loop for alll the columns in the dataframe \r\n",
        "    for col in df.columns:\r\n",
        "        #get the datatype of the column\r\n",
        "        col_type = df[col].dtypes\r\n",
        "        #if the data type is numeric then only start changing the datatype\r\n",
        "        #as it isnt much helpful for other data types \r\n",
        "        if col_type in numerics:\r\n",
        "            #stores the min value of the column \r\n",
        "            c_min = df[col].min()\r\n",
        "            #stores the maximum value of the column\r\n",
        "            c_max = df[col].max()\r\n",
        "            #for int type numerics\r\n",
        "            if str(col_type)[:3] == 'int':\r\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\r\n",
        "                    df[col] = df[col].astype(np.int8)\r\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\r\n",
        "                    df[col] = df[col].astype(np.int16)\r\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\r\n",
        "                    df[col] = df[col].astype(np.int32)\r\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\r\n",
        "                    df[col] = df[col].astype(np.int64)  \r\n",
        "            #for float type numerics \r\n",
        "            else:\r\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\r\n",
        "                    df[col] = df[col].astype(np.float16)\r\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\r\n",
        "                    df[col] = df[col].astype(np.float32)\r\n",
        "                else:\r\n",
        "                    df[col] = df[col].astype(np.float64)    \r\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\r\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n",
        "    return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fLiMoPlxbJH"
      },
      "source": [
        "## One hot encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FR9JP6YBCqN"
      },
      "source": [
        "def lab_enc(df, cols, prefix=''):\r\n",
        "  '''\r\n",
        "  categorical to numerical \r\n",
        "  label encode the values in the specified columns and \r\n",
        "  return the data frame\r\n",
        "  Parameters\r\n",
        "  -------------------\r\n",
        "  df   - Original DataFrame\r\n",
        "  cols - label encode the specified columns\r\n",
        "  -------------------\r\n",
        "  '''\r\n",
        "  \r\n",
        "  lbl_enc = LabelEncoder()\r\n",
        "  for col in cols:\r\n",
        "    df[col] = lbl_enc.fit_transform(df[col].astype(str))\r\n",
        "    np.save('{}_{}_enc.npy'.format(prefix, col), lbl_enc.classes_)\r\n",
        "  return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjIqob7HAElV"
      },
      "source": [
        "#Refer:-https://stackoverflow.com/questions/36631163/what-are-the-pros-and-cons-between-get-dummies-pandas-and-onehotencoder-sciki\r\n",
        "def one_hot_encoder(df, nan_as_category=True):\r\n",
        "    \"\"\"used to create the one hot encoding of the categorical variables \"\"\"\r\n",
        "    original_columns = list(df.columns)\r\n",
        "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\r\n",
        "    print(\"The columns on which one hot encoding is performed is \",categorical_columns)\r\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\r\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\r\n",
        "    return df, new_columns"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG9e1lLecnBx"
      },
      "source": [
        "## Time Based Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GINy-Khvcqjz"
      },
      "source": [
        "def get_basic_time_feat(df, grpby, col, s):\r\n",
        "  '''\r\n",
        "  create basic time feats like differece in minute, days etce tera\r\n",
        "  and return the dataframe.\r\n",
        "  \r\n",
        "  Parameters\r\n",
        "  ---------------------\r\n",
        "  df      - Features will be created\r\n",
        "  grpby   - group the DF based on this value\r\n",
        "  col     - column where the operations will be performed\r\n",
        "  s       - shift value\r\n",
        "  ---------------------\r\n",
        "  '''\r\n",
        "\r\n",
        "  df = df.sort_values(col)\r\n",
        "  for i in range(s):\r\n",
        "    df['prev_{}_'.format(i+1)+col] = df.groupby([grpby])[col].shift(i+1)\r\n",
        "    df['purchase_date_diff_{}_days'.format(i+1)] = (df[col] - df['prev_{}_'.format(i+1)+col]).dt.days.values\r\n",
        "    df['purchase_date_diff_{}_seconds'.format(i+1)] = df['purchase_date_diff_{}_days'.format(i+1)].values * 24 * 3600\r\n",
        "    df['purchase_date_diff_{}_seconds'.format(i+1)] += (df[col] - df['prev_{}_'.format(i+1)+col]).dt.seconds.values\r\n",
        "    df['purchase_date_diff_{}_hours'.format(i+1)] = df.iloc[:, -1].values // 3600\r\n",
        "\r\n",
        "  return df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BthVAhRwcrwn"
      },
      "source": [
        "## Group Aggregations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTSHb_v2cqbh"
      },
      "source": [
        "def s_agg(new_df, df, op, prefix, grpby, col):\r\n",
        "  '''\r\n",
        "  takes the data frame as input and return the dataframe with the aggregate operations performed.\r\n",
        "  \r\n",
        "  Parameters\r\n",
        "  ----------------------------\r\n",
        "  new_df  - DF with new features added\r\n",
        "  df      - original DF\r\n",
        "  op      - statistical operations like min, max, mean etc.\r\n",
        "  prefix  - prefix for the feature name\r\n",
        "  grpby   - based on which column to group by\r\n",
        "  col     - operations will be performed on this column\r\n",
        "  ----------------------------\r\n",
        "  '''\r\n",
        "\r\n",
        "  for o in op:\r\n",
        "    new_df[prefix+col+'_{}'.format(o)] = df.groupby([grpby])[col].agg([o]).values\r\n",
        "  return new_df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nozaOy-Lc2vY"
      },
      "source": [
        "\r\n",
        "def find_single_val(new_df, df, col, grpby, op, name='',  prefix='', use_col=False):\r\n",
        "  '''\r\n",
        "  find a value like min, max, mean in the specified column and return the DF\r\n",
        "  \r\n",
        "  Parameters\r\n",
        "  ------------------\r\n",
        "  new_df   - features will be added to this DF\r\n",
        "  df       - original DF from which the features will be created\r\n",
        "  col      - operations will be performed on this column\r\n",
        "  grpby    - based on this column we'll to group by\r\n",
        "  name     - name for the new features created\r\n",
        "  op       - statistical operations to be performed\r\n",
        "  prefix   - added to the name of the feature -- default value empty\r\n",
        "  use_col  - if set True then the original column name will be uesd to name the new feature -- default value False\r\n",
        "  ------------------\r\n",
        "  '''\r\n",
        "  \r\n",
        "  if use_col:\r\n",
        "    for c in col:\r\n",
        "      for o in op:\r\n",
        "        if o is 'min':\r\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].min().values\r\n",
        "        elif o is 'max':\r\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].max().values\r\n",
        "        elif o is 'mean':\r\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].mean().values\r\n",
        "        elif o is 'sum':\r\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].sum().values\r\n",
        "        elif o is 'nunique':\r\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].nunique().values\r\n",
        "        elif o is 'std':\r\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].std().values\r\n",
        "        elif o is 'count':\r\n",
        "          new_df[prefix+'_'+c+'_'+'{}'.format(o)] = df.groupby([grpby])[c].count().values\r\n",
        "\r\n",
        "  else:\r\n",
        "    for c in col:\r\n",
        "      for o in op:\r\n",
        "        if o is 'min':\r\n",
        "          new_df[name] = df.groupby([grpby])[c].min().values\r\n",
        "        elif o is 'max':\r\n",
        "          new_df[name] = df.groupby([grpby])[c].max().values\r\n",
        "        elif o is 'mean':\r\n",
        "          new_df[name] = df.groupby([grpby])[c].mean().values\r\n",
        "        elif o is 'sum':\r\n",
        "          new_df[name] = df.groupby([grpby])[c].sum().values\r\n",
        "        elif o is 'nunique':\r\n",
        "          new_df[name] = df.groupby([grpby])[c].nunique().values\r\n",
        "        elif o is 'std':\r\n",
        "          new_df[name] = df.groupby([grpby])[c].std().values\r\n",
        "        elif o is 'count':\r\n",
        "          new_df[name] = df.groupby([grpby])[c].count().values\r\n",
        "\r\n",
        "  return new_df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGHytQlDdXsc"
      },
      "source": [
        "#https://www.kaggle.com/fabiendaniel/elo-world?scriptVersionId=8335387\r\n",
        "def successive_aggregates(df, field1, field2):\r\n",
        "    '''\r\n",
        "    what this function does is that it group the data twice and find\r\n",
        "    basic aggregate values.\r\n",
        "    First it will goup by card_id and all the specified column one by one.\r\n",
        "    Then it will find the agg values like mean, min, max and std\r\n",
        "    for the purchase amount for each group.\r\n",
        "    Parameters\r\n",
        "    -------------------\r\n",
        "    df      - original DataFrame\r\n",
        "    field1  - first groupby along with card_id\r\n",
        "    field2  - second grouby along with card_id\r\n",
        "    -------------------\r\n",
        "    '''\r\n",
        "\r\n",
        "    t = df.groupby(['card_id', field1])[field2].mean()\r\n",
        "    u = pd.DataFrame(t).reset_index().groupby('card_id')[field2].agg(['mean', 'min', 'max', 'std'])\r\n",
        "    u.columns = ['new_transac_' + field1 + '_' + field2 + '_' + col for col in u.columns.values]\r\n",
        "    u.reset_index(inplace=True)\r\n",
        "    return u"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5c_pamPde62"
      },
      "source": [
        "## Get Influencial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZX95-hdXpM"
      },
      "source": [
        "def get_influential(df, col_name, date):\r\n",
        "  '''\r\n",
        "  This function return whether a purchase is influential or not.\r\n",
        "  A purchase is considered influential if it is made 100 days before a festival.\r\n",
        "  If it is not influential it will give a value 0 else the actual value.\r\n",
        "  Parameters\r\n",
        "  --------------------------------\r\n",
        "  df       - Dataframe where the operations will be performed\r\n",
        "  col_name - name of the new feature\r\n",
        "  date     - on which date the holiday is occuring\r\n",
        "  --------------------------------\r\n",
        "  '''\r\n",
        "\r\n",
        "  df[col_name] = (pd.to_datetime(date) - pd.to_datetime(df['purchase_date'])).dt.days\r\n",
        "  df[col_name] = df[col_name].apply(lambda x: x if x > 0 and x < 100 else 0)\r\n",
        "  return df"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9eKcrMJxdYw"
      },
      "source": [
        "## FE on Train Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nub7opBCTFbH"
      },
      "source": [
        "def train_test(num_rows=None):\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "   FUNCTION:\r\n",
        "        MERGE THE TRAIN AND TEST AND DOES SOME BASIC FEATURE ENGINEERING \r\n",
        "\r\n",
        "   ARGS:\r\n",
        "        NUM_ROWS: TO LOAD THE NUMBER OF ROWS FOR THE DATAFRAME(DEBUGGING)\r\n",
        "        DEFAULT IS NONE\r\n",
        "  \r\n",
        "   RETURNS:\r\n",
        "        THE MERGED DATAFRAME OF TRAIN AND TEST ALONG WITH BASIC FEATURE ENGINEERING \r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # load csv\r\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/data/train.csv', index_col=['card_id'], nrows=num_rows)\r\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/data/test.csv', index_col=['card_id'], nrows=num_rows)\r\n",
        "\r\n",
        "    print(\"Train samples: {}, test samples: {}\".format(len(train_df), len(test_df)))\r\n",
        "    OUTLIER_THRESHOLD = 30\r\n",
        "    # Create an outliers column set to 1 for\r\n",
        "    train_df['outliers'] = np.where(train_df['target'] < OUTLIER_THRESHOLD, 1, 0)\r\n",
        "\r\n",
        "    # set target as nan\r\n",
        "    test_df['target'] = np.nan\r\n",
        "\r\n",
        "    # merge\r\n",
        "    df = train_df.append(test_df)\r\n",
        "\r\n",
        "    del train_df, test_df\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    # to datetime\r\n",
        "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\r\n",
        "\r\n",
        "    # datetime features\r\n",
        "    df['quarter'] = df['first_active_month'].dt.quarter\r\n",
        "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\r\n",
        "    df['quarter_first_active_month'] = df['first_active_month'].dt.quarter\r\n",
        "    df['first_active_month_diff_from_today'] = (datetime.datetime.today() - df['first_active_month']).dt.days\r\n",
        "\r\n",
        "\r\n",
        "    # one hot encoding\r\n",
        "    df, cols = one_hot_encoder(df, nan_as_category=False)\r\n",
        "\r\n",
        "    for col in ['feature_1', 'feature_2', 'feature_3']:\r\n",
        "        order_label = df.groupby(col)['outliers'].mean()\r\n",
        "        df[col] = df[col].map(order_label)\r\n",
        "\r\n",
        "    # Some basic statistics transformations over the feature_i columns\r\n",
        "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\r\n",
        "    df['feature_mean'] = df['feature_sum'] / 3\r\n",
        "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\r\n",
        "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\r\n",
        "    df['feature_std'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\r\n",
        "\r\n",
        "    return df"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIQSUn8pxgTX"
      },
      "source": [
        "## Imputations on transactions data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zBpC472TFVT"
      },
      "source": [
        "def transactions_imputations(df_name='hist',nrows = None):\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    FUNCTION:\r\n",
        "          PERFORMS THE IMPUTATIONS FOR THE TRANSACTIONS DATA \r\n",
        "          IT HANDLES MISSING VALUES \r\n",
        "          IT HANDLES MISSING VALUES BY MODEL IMPUTATIONS \r\n",
        "          IT DOES MAPPING OF CATEGORICAL DATA TO NUMERICAL \r\n",
        "\r\n",
        "    ARGS:\r\n",
        "          DF:- THE DATAFRAME THAT NEEDS TO HAVE IMPUTATIONS \r\n",
        "          DEFAULT IS HISTORICAL TRANSACTIONS \r\n",
        "          NUM_ROWS: TO LOAD THE NUMBER OF ROWS FOR THE DATAFRAME(DEBUGGING)\r\n",
        "          DEFAULT IS NONE\r\n",
        "\r\n",
        "    RETURNS:\r\n",
        "          THE MODIFIED DATA FRAME WITH IMPUTATIONS \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    import pickle\r\n",
        "    from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "\r\n",
        "    if df_name == 'hist':\r\n",
        "      df = pd.read_csv('/content/drive/MyDrive/data/historical_transactions.csv', nrows=nrows)\r\n",
        "    else:\r\n",
        "      df = pd.read_csv('/content/drive/MyDrive/data/new_merchant_transactions.csv', nrows=nrows)\r\n",
        "    \r\n",
        "\r\n",
        "    # reduce memory usage\r\n",
        "    df = reduce_mem_usage(df)\r\n",
        "\r\n",
        "    print(df.isna().sum())\r\n",
        "\r\n",
        "    #category 2 and 3 have missing values and we will handle them via model based impuation \r\n",
        "\r\n",
        "    #creating a dataframe \r\n",
        "    temp = pd.DataFrame()\r\n",
        "\r\n",
        "    #get the card id\r\n",
        "    temp['card_id'] = df['card_id']\r\n",
        "\r\n",
        "    #get the merchant id\r\n",
        "    temp['merchant_id'] = df['merchant_id']\r\n",
        "\r\n",
        "    #get the purchase date\r\n",
        "    temp['purchase_date'] = df['purchase_date']\r\n",
        "\r\n",
        "    #drop the columns from the new merchant data frame \r\n",
        "    df.drop(['card_id', 'merchant_id', 'purchase_date'], axis=1, inplace=True)\r\n",
        "\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    #store all the columns as features \r\n",
        "    feat = df.columns\r\n",
        "\r\n",
        "    #having missing values \r\n",
        "    cols = ['category_2', 'category_3']\r\n",
        "\r\n",
        "    #label encode the variables\r\n",
        "    df = lab_enc(df, ['authorized_flag','category_1'], prefix=df_name)\r\n",
        "    print(df.sample(10))\r\n",
        "\r\n",
        "    #list to hold the null values\r\n",
        "    list_no_nan = []\r\n",
        "\r\n",
        "    #select only columns which doesn't have any null values\r\n",
        "    for c in feat:\r\n",
        "      if c not in cols:\r\n",
        "        list_no_nan.append(c)\r\n",
        "\r\n",
        "    #create a test set by selecting only rows which are having null values\r\n",
        "    #we will predict the values of categority 2\r\n",
        "    test = df[df['category_2'].isna()]\r\n",
        "\r\n",
        "    #create train set by selecting rows which doesn't have any null values\r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    d = {'A':1, 'B':2, 'C':3}\r\n",
        "    train['category_3'] = train['category_3'].map(d)\r\n",
        "    test['category_3'] = test['category_3'].map(d)\r\n",
        "\r\n",
        "    #fit the classifier to the train data\r\n",
        "    clf_name = df_name + 'LR1'\r\n",
        "    print(\"[INFO] NAME OF CLF \",clf_name )\r\n",
        "    clf_name = LogisticRegression()\r\n",
        "    clf_name.fit(train[list_no_nan], train['category_2'])\r\n",
        "    #save the model in pickel \r\n",
        "    pickle.dump(clf_name, open('clf_name.sav', 'wb'))\r\n",
        "\r\n",
        "    print(\"[INFO] Imputing Categorical 2 values ....\")\r\n",
        "    #make prediction only for the rows with null value\r\n",
        "    df.loc[df['category_2'].isna(), 'category_2'] = clf_name.predict(test[list_no_nan])\r\n",
        "\r\n",
        "    test = df[df['category_3'].isna()]\r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    clf_name = df_name + 'LR2'\r\n",
        "    print(\"[INFO] NAME OF CLF \",clf_name )\r\n",
        "    clf_name = LogisticRegression()\r\n",
        "    clf_name.fit(train[list_no_nan], train['category_3'])\r\n",
        "    #save the model in pickel\r\n",
        "    pickle.dump(clf_name, open('clf_name.sav', 'wb'))\r\n",
        "\r\n",
        "    print(\"[INFO] Imputing Categorical 3 values ....\")\r\n",
        "    df.loc[df['category_3'].isna(), 'category_3'] = clf_name.predict(test[list_no_nan])\r\n",
        "\r\n",
        "    df['card_id'] = temp['card_id']\r\n",
        "    df['merchant_id'] = temp['merchant_id']\r\n",
        "    df['purchase_date'] = temp['purchase_date']\r\n",
        "\r\n",
        "    print(df.isna().sum())\r\n",
        "    print(\"[INFO] SAVING ...\")\r\n",
        "    file_name = df_name\r\n",
        "    df.to_csv('file_name.csv', index=False)\r\n",
        "\r\n",
        "\r\n",
        "    return df\r\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Scb5_LjxlFl"
      },
      "source": [
        "## Imputations on Merchant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOveUvGDZ-7y"
      },
      "source": [
        "def merchant_imputations(num_rows=None):\r\n",
        "    \"\"\"\r\n",
        "    FUNCTION:\r\n",
        "          PERFORMS THE IMPUTATIONS FOR THE TRANSACTIONS DATA \r\n",
        "          IT HANDLES MISSING VALUES \r\n",
        "          IT DOES MAPPING OF CATEGORICAL DATA TO NUMERICAL\r\n",
        "          IT HANDLES THE INF VALUES  \r\n",
        "\r\n",
        "    ARGS:\r\n",
        "          NUM_ROWS: TO LOAD THE NUMBER OF ROWS FOR THE DATAFRAME(DEBUGGING)\r\n",
        "          DEFAULT IS NONE\r\n",
        "    \r\n",
        "    RETURNS:\r\n",
        "          THE MODIFIED DATA FRAME WITH IMPUTATIONS \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    import pickle\r\n",
        "    from sklearn.linear_model import LogisticRegression\r\n",
        "    from sklearn.neighbors import KNeighborsRegressor\r\n",
        "\r\n",
        "    df = pd.read_csv('/content/drive/MyDrive/data/merchants.csv', nrows=num_rows)\r\n",
        "    df = reduce_mem_usage(df)\r\n",
        "\r\n",
        "    #get all the ones not inf\r\n",
        "    #suprisingly the 3 values have the nan and inf\r\n",
        "    df = df[df['avg_purchases_lag3']!=np.inf]\r\n",
        "\r\n",
        "    #creating a temporary dataframe \r\n",
        "    temp = pd.DataFrame()\r\n",
        "    temp['merchant_id'] = df['merchant_id']\r\n",
        "    # we use KNN for numerical and logisticregrssion for categorical \r\n",
        "    temp['category_2'] = df['category_2']\r\n",
        "\r\n",
        "    #category 2 is needed for 2nd round for logistic regression\r\n",
        "    # merchant id is not useful\r\n",
        "    df.drop(['merchant_id', 'category_2'], axis=1, inplace=True)\r\n",
        "\r\n",
        "    #categorical to numerical \r\n",
        "    df = lab_enc(df, ['category_4','category_1','most_recent_sales_range','most_recent_purchases_range'], prefix='merchant')\r\n",
        "\r\n",
        "    features = df.columns\r\n",
        "    #sales have missing values \r\n",
        "    cols = ['avg_sales_lag3','avg_sales_lag6','avg_sales_lag12']\r\n",
        "    list_no_nan = []\r\n",
        "\r\n",
        "    #get the features not to predict and having no nan \r\n",
        "    for c in features:\r\n",
        "      if c not in cols:\r\n",
        "        list_no_nan.append(c)\r\n",
        "\r\n",
        "    #have all nan as we need to predict this (avg_sales_lag3)\r\n",
        "    test = df[df['avg_sales_lag3'].isna()]\r\n",
        "    #have no nan to train \r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    #initilize for knn \r\n",
        "    knn_sal_3 = KNeighborsRegressor(n_neighbors=5)\r\n",
        "\r\n",
        "    #fit on data \r\n",
        "    knn_sal_3.fit(train[list_no_nan], train['avg_sales_lag3'])\r\n",
        "    #fill the missing values \r\n",
        "    df.loc[df['avg_sales_lag3'].isna(), 'avg_sales_lag3'] = knn_sal_3.predict(test[list_no_nan])\r\n",
        "    #save the model \r\n",
        "    pickle.dump(knn_sal_3, open('knn_sal_3.sav', 'wb'))\r\n",
        "\r\n",
        "    #have all nan as we need to predict this (avg_sales_lag6)\r\n",
        "    test = df[df['avg_sales_lag6'].isna()]\r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    #initilize the model\r\n",
        "    knn_sal_6 = KNeighborsRegressor(n_neighbors=5)\r\n",
        "    #fit the model \r\n",
        "    knn_sal_6.fit(train[list_no_nan], train['avg_sales_lag6'])\r\n",
        "    #fill the missing values \r\n",
        "    df.loc[df['avg_sales_lag6'].isna(), 'avg_sales_lag6'] = knn_sal_6.predict(test[list_no_nan])\r\n",
        "    #save the model \r\n",
        "    pickle.dump(knn_sal_6, open('knn_sal_6.sav', 'wb'))\r\n",
        "\r\n",
        "    #have all nan as we need to predict this (avg_sales_lag12)\r\n",
        "    test = df[df['avg_sales_lag12'].isna()]\r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    #initilize the model\r\n",
        "    knn_sal_12 = KNeighborsRegressor(n_neighbors=5)\r\n",
        "    #fit on data \r\n",
        "    knn_sal_12.fit(train[list_no_nan], train['avg_sales_lag12'])\r\n",
        "    #fill the missing values \r\n",
        "    df.loc[df['avg_sales_lag12'].isna(), 'avg_sales_lag12'] = knn_sal_12.predict(test[list_no_nan])\r\n",
        "    #save the model \r\n",
        "    pickle.dump(knn_sal_12, open('knn_sal_12.sav', 'wb'))\r\n",
        "\r\n",
        "    #for category 2\r\n",
        "    df['category_2'] = temp['category_2']\r\n",
        "\r\n",
        "    feat = df.columns\r\n",
        "    cols = ['category_2']\r\n",
        "    list_no_nan = []\r\n",
        "\r\n",
        "    #get the features not to predict and having no nan \r\n",
        "    for c in feat:\r\n",
        "      if c not in cols:\r\n",
        "        list_no_nan.append(c)\r\n",
        "\r\n",
        "    #get all the missing values \r\n",
        "    test = df[df['category_2'].isna()]\r\n",
        "    # have all the good values \r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    LR_cat_2 = LogisticRegression()\r\n",
        "    LR_cat_2.fit(train[list_no_nan], train['category_2'])\r\n",
        "    df.loc[df['category_2'].isna(), 'category_2'] = LR_cat_2.predict(test[list_no_nan])\r\n",
        "    pickle.dump(LR_cat_2, open('LR_cat_2.sav', 'wb'))\r\n",
        "\r\n",
        "    df['merchant_id'] = temp['merchant_id']\r\n",
        "\r\n",
        "    return df"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQxMKqamxvY5"
      },
      "source": [
        "## Generating the aurgumentated dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljbF5QxSTFEC"
      },
      "source": [
        "def imputations_mer_trans(debug=False):\r\n",
        "\r\n",
        "    \"\"\" \r\n",
        "    FUNCTION:\r\n",
        "      Do model based imputation \r\n",
        "      STEPS:-\r\n",
        "      1. Load and Impute Historical Transactions\r\n",
        "      2. Load and Impute New Merchants Transactions\r\n",
        "      3. Load and Impute  Merchants   \r\n",
        "\r\n",
        "    ARGS:\r\n",
        "     Debug:- Takes care of numer of rows as we need faster manipulation of data in dubigging \r\n",
        "\r\n",
        "    RETURNS:\r\n",
        "      Generates the aurgumented train and test \r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    num_rows = 10000 if debug else None\r\n",
        "    with timer(\"load and impute transactions ....\"):\r\n",
        "        # load AND IMPUTE \r\n",
        "        print(\"[INFO] LOADING AND IMPUTING HISTORICAL TRANSACTIONS ....\")\r\n",
        "        historical_transactions_df = transactions_imputations(df_name='hist',nrows = num_rows)\r\n",
        "        print(\"[INFO] LOADING AND IMPUTING HISTORICAL TRANSACTIONS ....\")\r\n",
        "        new_merchant_df = transactions_imputations(df_name='mer',nrows = num_rows)\r\n",
        "\r\n",
        "        #unique values of column of card id \r\n",
        "        print(\"[INFO] THE UNIQUE VALUES OF CARD ID ...\")\r\n",
        "        print(\"FOR NEW MERCHANT \",new_merchant_df.card_id.nunique())\r\n",
        "        print(\"FOR HISTORICAL TRANSACTIONS \",historical_transactions_df.card_id.nunique())\r\n",
        "        print(\"[INFO] Check for Nan\")\r\n",
        "        print(historical_transactions_df.isna().sum())\r\n",
        "        print(historical_transactions_df.isna().sum())\r\n",
        "\r\n",
        "    with timer(\"load and impute merchants ....\"):\r\n",
        "        #load csv \r\n",
        "        print(\"[INFO] LOADING MERCHANTS AND IMPUTATIONS  ....\")\r\n",
        "        merchant_df = merchant_imputations(num_rows=num_rows)\r\n",
        "\r\n",
        "        print(\"[INFO] Check for Nan ...\")\r\n",
        "        print(merchant_df.isna().sum())    \r\n",
        "\r\n",
        "        return new_merchant_df, historical_transactions_df, merchant_df"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-evMw4g8xzS4"
      },
      "source": [
        "## Main Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yonYSWj_jIU",
        "outputId": "b0cec696-05d2-4729-ed8f-e6eada17e681"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "  new_merchant_df,historical_transactions_df,merchant_df = imputations_mer_trans(debug=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] LOADING AND IMPUTING HISTORICAL TRANSACTIONS ....\n",
            "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
            "authorized_flag               0\n",
            "card_id                       0\n",
            "city_id                       0\n",
            "category_1                    0\n",
            "installments                  0\n",
            "category_3               178159\n",
            "merchant_category_id          0\n",
            "merchant_id              138481\n",
            "month_lag                     0\n",
            "purchase_amount               0\n",
            "purchase_date                 0\n",
            "category_2              2652864\n",
            "state_id                      0\n",
            "subsector_id                  0\n",
            "dtype: int64\n",
            "          authorized_flag  city_id  ...  state_id  subsector_id\n",
            "2163503                 1       20  ...        19            37\n",
            "12951525                1      291  ...         9            27\n",
            "9215813                 1       69  ...         9            34\n",
            "15600977                1      158  ...        15            27\n",
            "2312213                 1      313  ...         5            27\n",
            "1712986                 1       69  ...         9            33\n",
            "27767167                1       20  ...        19            33\n",
            "18214840                1      199  ...        14            33\n",
            "4682885                 1       69  ...         9            33\n",
            "6945543                 0       17  ...        22            16\n",
            "\n",
            "[10 rows x 11 columns]\n",
            "[INFO] NAME OF CLF  histLR1\n",
            "[INFO] Imputing Categorical 2 values ....\n",
            "[INFO] NAME OF CLF  histLR2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0HuMagaNrow"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}