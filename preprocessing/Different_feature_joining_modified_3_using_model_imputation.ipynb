{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Different_feature_joining_modified_3_using_model_imputation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/human-ai2025/Elo-Merchant-Recommendation/blob/master/preprocessing/Different_feature_joining_modified_3_using_model_imputation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oeG4uYVxOap"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSNUHQTd_jbt"
      },
      "source": [
        "#Import Libraries \r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "import gc\r\n",
        "from contextlib import contextmanager\r\n",
        "from pandas.core.common import SettingWithCopyWarning\r\n",
        "import datetime\r\n",
        "import time\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "from scipy.stats import mode\r\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTKdW0Kx8ku"
      },
      "source": [
        "### Mount the drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0gy8in1_jYK",
        "outputId": "0b9d7c38-6c50-4b57-eb3c-3a39f9f38242"
      },
      "source": [
        "#Mounting drive \r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjAQaUCAx-7P"
      },
      "source": [
        "### Setup the current path "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG9WyuKGyWx-",
        "outputId": "f8d63b8f-f9df-4c88-9c3d-2bab11722910"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feJTkhbC_jVa",
        "outputId": "bb2d994c-aad7-43c5-8fc0-32f9e36bf3b6"
      },
      "source": [
        "#Setting up worksapce directory \r\n",
        "%cd /content/drive/MyDrive/data "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQIFTxv1ydQ7",
        "outputId": "2bd2077d-0402-4cc4-bc17-3931e984407f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " augmented_test.csv\n",
            " augmented_test_try1.csv\n",
            " augmented_train.csv\n",
            " augmented_train_try1.csv\n",
            " clf_cat2.sav\n",
            " clf_cat3.sav\n",
            "'Data Dictionary.xlsx'\n",
            " Data_Dictionary.xlsx\n",
            " historical_transactions.csv\n",
            " historical_transactions_df.pkl\n",
            " merchant_category_1_enc.npy\n",
            " merchant_category_4_enc.npy\n",
            " merchant_most_recent_purchases_range_enc.npy\n",
            " merchant_most_recent_sales_range_enc.npy\n",
            " merchants.csv\n",
            " merged_transactions_with_merchants.csv\n",
            " new_merchant_authorized_flag_enc.npy\n",
            " new_merchant_category_1_enc.npy\n",
            " new_merchant_df.pkl\n",
            " new_merchant_transactions.csv\n",
            " sample_submission.csv\n",
            " test.csv\n",
            " train.csv\n",
            " transactions_raw_merged.csv\n",
            " transactions_refined_1_merged_WON.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXBfyhiOxWKf"
      },
      "source": [
        "## Timer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxDmUU0b_jSw"
      },
      "source": [
        "#refer:-https://www.youtube.com/watch?v=vOMtQ4ocMGI\r\n",
        "@contextmanager\r\n",
        "def timer(title):\r\n",
        "    \"\"\" used to calculate time for each function\"\"\"\r\n",
        "    t0 = time.time()\r\n",
        "    yield\r\n",
        "    print(\"{} - done in {:.000f}s\".format(title, time.time() - t0))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjkNfzYyxX_3"
      },
      "source": [
        "## Memory Reduction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiFE8Io2_jQM"
      },
      "source": [
        "#https://www.kaggle.com/fabiendaniel/elo-world\r\n",
        "#Function to load data into pandas and reduce memory usage\r\n",
        "\r\n",
        "def reduce_mem_usage(df, verbose=True):\r\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\r\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \r\n",
        "    #loop for alll the columns in the dataframe \r\n",
        "    for col in df.columns:\r\n",
        "        #get the datatype of the column\r\n",
        "        col_type = df[col].dtypes\r\n",
        "        #if the data type is numeric then only start changing the datatype\r\n",
        "        #as it isnt much helpful for other data types \r\n",
        "        if col_type in numerics:\r\n",
        "            #stores the min value of the column \r\n",
        "            c_min = df[col].min()\r\n",
        "            #stores the maximum value of the column\r\n",
        "            c_max = df[col].max()\r\n",
        "            #for int type numerics\r\n",
        "            if str(col_type)[:3] == 'int':\r\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\r\n",
        "                    df[col] = df[col].astype(np.int8)\r\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\r\n",
        "                    df[col] = df[col].astype(np.int16)\r\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\r\n",
        "                    df[col] = df[col].astype(np.int32)\r\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\r\n",
        "                    df[col] = df[col].astype(np.int64)  \r\n",
        "            #for float type numerics \r\n",
        "            else:\r\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\r\n",
        "                    df[col] = df[col].astype(np.float16)\r\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\r\n",
        "                    df[col] = df[col].astype(np.float32)\r\n",
        "                else:\r\n",
        "                    df[col] = df[col].astype(np.float64)    \r\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\r\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n",
        "    return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fLiMoPlxbJH"
      },
      "source": [
        "## One hot encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FR9JP6YBCqN"
      },
      "source": [
        "def lab_enc(df, cols, prefix=''):\r\n",
        "  '''\r\n",
        "  categorical to numerical \r\n",
        "  df   - Original DataFrame\r\n",
        "  cols - label encode the specified columns\r\n",
        "  #https://towardsdatascience.com/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b#:~:text=Label%20Encoder%3A&text=LabelEncoder%20encode%20labels%20with%20a,value%20to%20as%20assigned%20earlier.&text=The%20categorical%20values%20have%20been,all%20label%20encoding%20is%20about.\r\n",
        "  '''\r\n",
        "  \r\n",
        "  lbl_enc = LabelEncoder()\r\n",
        "  for col in cols:\r\n",
        "    df[col] = lbl_enc.fit_transform(df[col].astype(str))\r\n",
        "    #https://numpy.org/doc/stable/reference/generated/numpy.save.html\r\n",
        "  return df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIQSUn8pxgTX"
      },
      "source": [
        "## Imputations on transactions data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zBpC472TFVT"
      },
      "source": [
        "def transactions_imputations(df_name='hist',nrows = None):\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    FUNCTION:\r\n",
        "          PERFORMS THE IMPUTATIONS FOR THE TRANSACTIONS DATA \r\n",
        "          IT HANDLES MISSING VALUES \r\n",
        "          IT HANDLES MISSING VALUES BY MODEL IMPUTATIONS \r\n",
        "          IT DOES MAPPING OF CATEGORICAL DATA TO NUMERICAL \r\n",
        "\r\n",
        "    ARGS:\r\n",
        "          DF:- THE DATAFRAME THAT NEEDS TO HAVE IMPUTATIONS \r\n",
        "          DEFAULT IS HISTORICAL TRANSACTIONS \r\n",
        "          NUM_ROWS: TO LOAD THE NUMBER OF ROWS FOR THE DATAFRAME(DEBUGGING)\r\n",
        "          DEFAULT IS NONE\r\n",
        "\r\n",
        "    RETURNS:\r\n",
        "          THE MODIFIED DATA FRAME WITH IMPUTATIONS \r\n",
        "\r\n",
        "    REFER:-https://medium.com/towards-artificial-intelligence/handling-missing-data-for-advanced-machine-learning-b6eb89050357\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    import pickle\r\n",
        "    from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "\r\n",
        "    if df_name == 'hist':\r\n",
        "      df = pd.read_csv('/content/drive/MyDrive/data/historical_transactions.csv', nrows=nrows)\r\n",
        "    else:\r\n",
        "      df = pd.read_csv('/content/drive/MyDrive/data/new_merchant_transactions.csv', nrows=nrows)\r\n",
        "    \r\n",
        "\r\n",
        "    # reduce memory usage\r\n",
        "    df = reduce_mem_usage(df)\r\n",
        "\r\n",
        "    print(df.isna().sum())\r\n",
        "\r\n",
        "    #category 2 and 3 have missing values and we will handle them via model based impuation \r\n",
        "\r\n",
        "    #creating a dataframe \r\n",
        "    temp = pd.DataFrame()\r\n",
        "\r\n",
        "    #get the card id\r\n",
        "    temp['card_id'] = df['card_id']\r\n",
        "\r\n",
        "    #get the merchant id\r\n",
        "    temp['merchant_id'] = df['merchant_id']\r\n",
        "\r\n",
        "    #get the purchase date\r\n",
        "    temp['purchase_date'] = df['purchase_date']\r\n",
        "\r\n",
        "    #drop the columns from the new merchant data frame \r\n",
        "    df.drop(['card_id', 'merchant_id', 'purchase_date'], axis=1, inplace=True)\r\n",
        "\r\n",
        "    gc.collect()\r\n",
        "\r\n",
        "    #store all the columns as features \r\n",
        "    feat = df.columns\r\n",
        "\r\n",
        "    #having missing values \r\n",
        "    cols = ['category_2', 'category_3']\r\n",
        "\r\n",
        "    #label encode the variables\r\n",
        "    df = lab_enc(df, ['authorized_flag','category_1'], prefix=df_name)\r\n",
        "    print(df.sample(10))\r\n",
        "\r\n",
        "    #list to hold the null values\r\n",
        "    list_no_nan = []\r\n",
        "\r\n",
        "    #select only columns which doesn't have any null values\r\n",
        "    for c in feat:\r\n",
        "      if c not in cols:\r\n",
        "        list_no_nan.append(c)\r\n",
        "\r\n",
        "    #create a test set by selecting only rows which are having null values\r\n",
        "    #we will predict the values of categority 2\r\n",
        "    test = df[df['category_2'].isna()]\r\n",
        "\r\n",
        "    #create train set by selecting rows which doesn't have any null values\r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    d = {'A':1, 'B':2, 'C':3}\r\n",
        "    train['category_3'] = train['category_3'].map(d)\r\n",
        "    test['category_3'] = test['category_3'].map(d)\r\n",
        "\r\n",
        "    #fit the classifier to the train data\r\n",
        "    clf_name = df_name + 'LR1'\r\n",
        "    print(\"[INFO] NAME OF CLF \",clf_name )\r\n",
        "    clf_name = LogisticRegression()\r\n",
        "    clf_name.fit(train[list_no_nan], train['category_2'])\r\n",
        "    #save the model in pickel \r\n",
        "    pickle.dump(clf_name, open('clf_name.sav', 'wb'))\r\n",
        "\r\n",
        "    print(\"[INFO] Imputing Categorical 2 values ....\")\r\n",
        "    #make prediction only for the rows with null value\r\n",
        "    df.loc[df['category_2'].isna(), 'category_2'] = clf_name.predict(test[list_no_nan])\r\n",
        "\r\n",
        "    test = df[df['category_3'].isna()]\r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    clf_name = df_name + 'LR2'\r\n",
        "    print(\"[INFO] NAME OF CLF \",clf_name )\r\n",
        "    clf_name = LogisticRegression()\r\n",
        "    clf_name.fit(train[list_no_nan], train['category_3'])\r\n",
        "    #save the model in pickel\r\n",
        "    pickle.dump(clf_name, open('clf_name.sav', 'wb'))\r\n",
        "\r\n",
        "    print(\"[INFO] Imputing Categorical 3 values ....\")\r\n",
        "    df.loc[df['category_3'].isna(), 'category_3'] = clf_name.predict(test[list_no_nan])\r\n",
        "\r\n",
        "    df['card_id'] = temp['card_id']\r\n",
        "    df['merchant_id'] = temp['merchant_id']\r\n",
        "    df['purchase_date'] = temp['purchase_date']\r\n",
        "\r\n",
        "    print(df.isna().sum())\r\n",
        "    print(\"[INFO] SAVING ...\")\r\n",
        "    file_name = df_name\r\n",
        "    df.to_csv('file_name.csv', index=False)\r\n",
        "\r\n",
        "\r\n",
        "    return df\r\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Scb5_LjxlFl"
      },
      "source": [
        "## Imputations on Merchant Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOveUvGDZ-7y"
      },
      "source": [
        "def merchant_imputations(num_rows=None):\r\n",
        "    \"\"\"\r\n",
        "    FUNCTION:\r\n",
        "          PERFORMS THE IMPUTATIONS FOR THE TRANSACTIONS DATA \r\n",
        "          IT HANDLES MISSING VALUES \r\n",
        "          IT DOES MAPPING OF CATEGORICAL DATA TO NUMERICAL\r\n",
        "          IT HANDLES THE INF VALUES  \r\n",
        "\r\n",
        "    ARGS:\r\n",
        "          NUM_ROWS: TO LOAD THE NUMBER OF ROWS FOR THE DATAFRAME(DEBUGGING)\r\n",
        "          DEFAULT IS NONE\r\n",
        "    \r\n",
        "    RETURNS:\r\n",
        "          THE MODIFIED DATA FRAME WITH IMPUTATIONS \r\n",
        "\r\n",
        "    REFER:-https://medium.com/towards-artificial-intelligence/handling-missing-data-for-advanced-machine-learning-b6eb89050357\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    import pickle\r\n",
        "    from sklearn.linear_model import LogisticRegression\r\n",
        "    from sklearn.neighbors import KNeighborsRegressor\r\n",
        "\r\n",
        "    df = pd.read_csv('/content/drive/MyDrive/data/merchants.csv', nrows=num_rows)\r\n",
        "    df = reduce_mem_usage(df)\r\n",
        "\r\n",
        "    #get all the ones not inf\r\n",
        "    #suprisingly the 3 values have the nan and inf\r\n",
        "    df = df[df['avg_purchases_lag3']!=np.inf]\r\n",
        "\r\n",
        "    #creating a temporary dataframe \r\n",
        "    temp = pd.DataFrame()\r\n",
        "    temp['merchant_id'] = df['merchant_id']\r\n",
        "    # we use KNN for numerical and logisticregrssion for categorical \r\n",
        "    temp['category_2'] = df['category_2']\r\n",
        "\r\n",
        "    #category 2 is needed for 2nd round for logistic regression\r\n",
        "    # merchant id is not useful\r\n",
        "    df.drop(['merchant_id', 'category_2'], axis=1, inplace=True)\r\n",
        "\r\n",
        "    #categorical to numerical \r\n",
        "    df = lab_enc(df, ['category_4','category_1','most_recent_sales_range','most_recent_purchases_range'], prefix='merchant')\r\n",
        "\r\n",
        "    features = df.columns\r\n",
        "    #sales have missing values \r\n",
        "    cols = ['avg_sales_lag3','avg_sales_lag6','avg_sales_lag12']\r\n",
        "    list_no_nan = []\r\n",
        "\r\n",
        "    #get the features not to predict and having no nan \r\n",
        "    for c in features:\r\n",
        "      if c not in cols:\r\n",
        "        list_no_nan.append(c)\r\n",
        "\r\n",
        "    #have all nan as we need to predict this (avg_sales_lag3)\r\n",
        "    test = df[df['avg_sales_lag3'].isna()]\r\n",
        "    #have no nan to train \r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    #initilize for knn \r\n",
        "    knn_sal_3 = KNeighborsRegressor(n_neighbors=5)\r\n",
        "\r\n",
        "    #fit on data \r\n",
        "    knn_sal_3.fit(train[list_no_nan], train['avg_sales_lag3'])\r\n",
        "    #fill the missing values \r\n",
        "    df.loc[df['avg_sales_lag3'].isna(), 'avg_sales_lag3'] = knn_sal_3.predict(test[list_no_nan])\r\n",
        "    #save the model \r\n",
        "    pickle.dump(knn_sal_3, open('knn_sal_3.sav', 'wb'))\r\n",
        "\r\n",
        "    #have all nan as we need to predict this (avg_sales_lag6)\r\n",
        "    test = df[df['avg_sales_lag6'].isna()]\r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    #initilize the model\r\n",
        "    knn_sal_6 = KNeighborsRegressor(n_neighbors=5)\r\n",
        "    #fit the model \r\n",
        "    knn_sal_6.fit(train[list_no_nan], train['avg_sales_lag6'])\r\n",
        "    #fill the missing values \r\n",
        "    df.loc[df['avg_sales_lag6'].isna(), 'avg_sales_lag6'] = knn_sal_6.predict(test[list_no_nan])\r\n",
        "    #save the model \r\n",
        "    pickle.dump(knn_sal_6, open('knn_sal_6.sav', 'wb'))\r\n",
        "\r\n",
        "    #have all nan as we need to predict this (avg_sales_lag12)\r\n",
        "    test = df[df['avg_sales_lag12'].isna()]\r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    #initilize the model\r\n",
        "    knn_sal_12 = KNeighborsRegressor(n_neighbors=5)\r\n",
        "    #fit on data \r\n",
        "    knn_sal_12.fit(train[list_no_nan], train['avg_sales_lag12'])\r\n",
        "    #fill the missing values \r\n",
        "    df.loc[df['avg_sales_lag12'].isna(), 'avg_sales_lag12'] = knn_sal_12.predict(test[list_no_nan])\r\n",
        "    #save the model \r\n",
        "    pickle.dump(knn_sal_12, open('knn_sal_12.sav', 'wb'))\r\n",
        "\r\n",
        "    #for category 2\r\n",
        "    df['category_2'] = temp['category_2']\r\n",
        "\r\n",
        "    feat = df.columns\r\n",
        "    cols = ['category_2']\r\n",
        "    list_no_nan = []\r\n",
        "\r\n",
        "    #get the features not to predict and having no nan \r\n",
        "    for c in feat:\r\n",
        "      if c not in cols:\r\n",
        "        list_no_nan.append(c)\r\n",
        "\r\n",
        "    #get all the missing values \r\n",
        "    test = df[df['category_2'].isna()]\r\n",
        "    # have all the good values \r\n",
        "    train = df.dropna()\r\n",
        "\r\n",
        "    LR_cat_2 = LogisticRegression()\r\n",
        "    LR_cat_2.fit(train[list_no_nan], train['category_2'])\r\n",
        "    df.loc[df['category_2'].isna(), 'category_2'] = LR_cat_2.predict(test[list_no_nan])\r\n",
        "    pickle.dump(LR_cat_2, open('LR_cat_2.sav', 'wb'))\r\n",
        "\r\n",
        "    df['merchant_id'] = temp['merchant_id']\r\n",
        "\r\n",
        "    return df"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQxMKqamxvY5"
      },
      "source": [
        "## Generating the aurgumentated dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljbF5QxSTFEC"
      },
      "source": [
        "def imputations_mer_trans(debug=False):\r\n",
        "\r\n",
        "    \"\"\" \r\n",
        "    FUNCTION:\r\n",
        "      Do model based imputation \r\n",
        "      STEPS:-\r\n",
        "      1. Load and Impute Historical Transactions\r\n",
        "      2. Load and Impute New Merchants Transactions\r\n",
        "      3. Load and Impute  Merchants   \r\n",
        "\r\n",
        "    ARGS:\r\n",
        "     Debug:- Takes care of numer of rows as we need faster manipulation of data in dubigging \r\n",
        "\r\n",
        "    RETURNS:\r\n",
        "      Generates the aurgumented train and test \r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    num_rows = 10000 if debug else None\r\n",
        "    with timer(\"load and impute transactions ....\"):\r\n",
        "        # load AND IMPUTE \r\n",
        "        print(\"[INFO] LOADING AND IMPUTING HISTORICAL TRANSACTIONS ....\")\r\n",
        "        historical_transactions_df = transactions_imputations(df_name='hist',nrows = num_rows)\r\n",
        "        print(\"[INFO] LOADING AND IMPUTING HISTORICAL TRANSACTIONS ....\")\r\n",
        "        new_merchant_df = transactions_imputations(df_name='mer',nrows = num_rows)\r\n",
        "\r\n",
        "        #unique values of column of card id \r\n",
        "        print(\"[INFO] THE UNIQUE VALUES OF CARD ID ...\")\r\n",
        "        print(\"FOR NEW MERCHANT \",new_merchant_df.card_id.nunique())\r\n",
        "        print(\"FOR HISTORICAL TRANSACTIONS \",historical_transactions_df.card_id.nunique())\r\n",
        "        print(\"[INFO] Check for Nan\")\r\n",
        "        print(historical_transactions_df.isna().sum())\r\n",
        "        print(historical_transactions_df.isna().sum())\r\n",
        "\r\n",
        "    with timer(\"load and impute merchants ....\"):\r\n",
        "        #load csv \r\n",
        "        print(\"[INFO] LOADING MERCHANTS AND IMPUTATIONS  ....\")\r\n",
        "        merchant_df = merchant_imputations(num_rows=num_rows)\r\n",
        "\r\n",
        "        print(\"[INFO] Check for Nan ...\")\r\n",
        "        print(merchant_df.isna().sum())    \r\n",
        "\r\n",
        "        return new_merchant_df, historical_transactions_df, merchant_df"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-evMw4g8xzS4"
      },
      "source": [
        "## Main Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yonYSWj_jIU",
        "outputId": "b0cec696-05d2-4729-ed8f-e6eada17e681"
      },
      "source": [
        "if __name__ == \"__main__\":\r\n",
        "  new_merchant_df,historical_transactions_df,merchant_df = imputations_mer_trans(debug=False)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] LOADING AND IMPUTING HISTORICAL TRANSACTIONS ....\n",
            "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
            "authorized_flag               0\n",
            "card_id                       0\n",
            "city_id                       0\n",
            "category_1                    0\n",
            "installments                  0\n",
            "category_3               178159\n",
            "merchant_category_id          0\n",
            "merchant_id              138481\n",
            "month_lag                     0\n",
            "purchase_amount               0\n",
            "purchase_date                 0\n",
            "category_2              2652864\n",
            "state_id                      0\n",
            "subsector_id                  0\n",
            "dtype: int64\n",
            "          authorized_flag  city_id  ...  state_id  subsector_id\n",
            "2163503                 1       20  ...        19            37\n",
            "12951525                1      291  ...         9            27\n",
            "9215813                 1       69  ...         9            34\n",
            "15600977                1      158  ...        15            27\n",
            "2312213                 1      313  ...         5            27\n",
            "1712986                 1       69  ...         9            33\n",
            "27767167                1       20  ...        19            33\n",
            "18214840                1      199  ...        14            33\n",
            "4682885                 1       69  ...         9            33\n",
            "6945543                 0       17  ...        22            16\n",
            "\n",
            "[10 rows x 11 columns]\n",
            "[INFO] NAME OF CLF  histLR1\n",
            "[INFO] Imputing Categorical 2 values ....\n",
            "[INFO] NAME OF CLF  histLR2\n",
            "[INFO] Imputing Categorical 3 values ....\n",
            "authorized_flag              0\n",
            "city_id                      0\n",
            "category_1                   0\n",
            "installments                 0\n",
            "category_3                   0\n",
            "merchant_category_id         0\n",
            "month_lag                    0\n",
            "purchase_amount              0\n",
            "category_2                   0\n",
            "state_id                     0\n",
            "subsector_id                 0\n",
            "card_id                      0\n",
            "merchant_id             138481\n",
            "purchase_date                0\n",
            "dtype: int64\n",
            "[INFO] SAVING ...\n",
            "[INFO] LOADING AND IMPUTING HISTORICAL TRANSACTIONS ....\n",
            "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n",
            "authorized_flag              0\n",
            "card_id                      0\n",
            "city_id                      0\n",
            "category_1                   0\n",
            "installments                 0\n",
            "category_3               55922\n",
            "merchant_category_id         0\n",
            "merchant_id              26216\n",
            "month_lag                    0\n",
            "purchase_amount              0\n",
            "purchase_date                0\n",
            "category_2              111745\n",
            "state_id                     0\n",
            "subsector_id                 0\n",
            "dtype: int64\n",
            "         authorized_flag  city_id  ...  state_id  subsector_id\n",
            "1843772                0      289  ...         5            37\n",
            "248357                 0       19  ...         9            33\n",
            "186728                 0       69  ...         9            19\n",
            "1955067                0       25  ...         7            33\n",
            "1730519                0      115  ...         9            16\n",
            "650782                 0       69  ...         9            29\n",
            "711965                 0      170  ...         8            19\n",
            "1887916                0      143  ...         5            27\n",
            "1688038                0      162  ...        20            32\n",
            "66563                  0       69  ...         9            19\n",
            "\n",
            "[10 rows x 11 columns]\n",
            "[INFO] NAME OF CLF  merLR1\n",
            "[INFO] Imputing Categorical 2 values ....\n",
            "[INFO] NAME OF CLF  merLR2\n",
            "[INFO] Imputing Categorical 3 values ....\n",
            "authorized_flag             0\n",
            "city_id                     0\n",
            "category_1                  0\n",
            "installments                0\n",
            "category_3                  0\n",
            "merchant_category_id        0\n",
            "month_lag                   0\n",
            "purchase_amount             0\n",
            "category_2                  0\n",
            "state_id                    0\n",
            "subsector_id                0\n",
            "card_id                     0\n",
            "merchant_id             26216\n",
            "purchase_date               0\n",
            "dtype: int64\n",
            "[INFO] SAVING ...\n",
            "[INFO] THE UNIQUE VALUES OF CARD ID ...\n",
            "FOR NEW MERCHANT  290001\n",
            "FOR HISTORICAL TRANSACTIONS  325540\n",
            "[INFO] Check for Nan\n",
            "authorized_flag              0\n",
            "city_id                      0\n",
            "category_1                   0\n",
            "installments                 0\n",
            "category_3                   0\n",
            "merchant_category_id         0\n",
            "month_lag                    0\n",
            "purchase_amount              0\n",
            "category_2                   0\n",
            "state_id                     0\n",
            "subsector_id                 0\n",
            "card_id                      0\n",
            "merchant_id             138481\n",
            "purchase_date                0\n",
            "dtype: int64\n",
            "authorized_flag              0\n",
            "city_id                      0\n",
            "category_1                   0\n",
            "installments                 0\n",
            "category_3                   0\n",
            "merchant_category_id         0\n",
            "month_lag                    0\n",
            "purchase_amount              0\n",
            "category_2                   0\n",
            "state_id                     0\n",
            "subsector_id                 0\n",
            "card_id                      0\n",
            "merchant_id             138481\n",
            "purchase_date                0\n",
            "dtype: int64\n",
            "load and impute transactions .... - done in 2932s\n",
            "[INFO] LOADING MERCHANTS AND IMPUTATIONS  ....\n",
            "Mem. usage decreased to 30.32 Mb (46.0% reduction)\n",
            "[INFO] Check for Nan ...\n",
            "merchant_group_id              0\n",
            "merchant_category_id           0\n",
            "subsector_id                   0\n",
            "numerical_1                    0\n",
            "numerical_2                    0\n",
            "category_1                     0\n",
            "most_recent_sales_range        0\n",
            "most_recent_purchases_range    0\n",
            "avg_sales_lag3                 0\n",
            "avg_purchases_lag3             0\n",
            "active_months_lag3             0\n",
            "avg_sales_lag6                 0\n",
            "avg_purchases_lag6             0\n",
            "active_months_lag6             0\n",
            "avg_sales_lag12                0\n",
            "avg_purchases_lag12            0\n",
            "active_months_lag12            0\n",
            "category_4                     0\n",
            "city_id                        0\n",
            "state_id                       0\n",
            "category_2                     0\n",
            "merchant_id                    0\n",
            "dtype: int64\n",
            "load and impute merchants .... - done in 39s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0HuMagaNrow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb2b811-22c0-4aeb-f098-79f781c0cbfb"
      },
      "source": [
        "new_merchant_df.isna().sum()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "authorized_flag             0\n",
              "city_id                     0\n",
              "category_1                  0\n",
              "installments                0\n",
              "category_3                  0\n",
              "merchant_category_id        0\n",
              "month_lag                   0\n",
              "purchase_amount             0\n",
              "category_2                  0\n",
              "state_id                    0\n",
              "subsector_id                0\n",
              "card_id                     0\n",
              "merchant_id             26216\n",
              "purchase_date               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_Y_6RVWuArj",
        "outputId": "ee6705d8-6f85-498e-aa4b-a7fa747d84c6"
      },
      "source": [
        "historical_transactions_df.isna().sum()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "authorized_flag              0\n",
              "city_id                      0\n",
              "category_1                   0\n",
              "installments                 0\n",
              "category_3                   0\n",
              "merchant_category_id         0\n",
              "month_lag                    0\n",
              "purchase_amount              0\n",
              "category_2                   0\n",
              "state_id                     0\n",
              "subsector_id                 0\n",
              "card_id                      0\n",
              "merchant_id             138481\n",
              "purchase_date                0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D82ZMl3uGrm",
        "outputId": "7b18d1db-1eda-4ce9-8218-4ea339140aa9"
      },
      "source": [
        "merchant_df.isna().sum()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "merchant_group_id              0\n",
              "merchant_category_id           0\n",
              "subsector_id                   0\n",
              "numerical_1                    0\n",
              "numerical_2                    0\n",
              "category_1                     0\n",
              "most_recent_sales_range        0\n",
              "most_recent_purchases_range    0\n",
              "avg_sales_lag3                 0\n",
              "avg_purchases_lag3             0\n",
              "active_months_lag3             0\n",
              "avg_sales_lag6                 0\n",
              "avg_purchases_lag6             0\n",
              "active_months_lag6             0\n",
              "avg_sales_lag12                0\n",
              "avg_purchases_lag12            0\n",
              "active_months_lag12            0\n",
              "category_4                     0\n",
              "city_id                        0\n",
              "state_id                       0\n",
              "category_2                     0\n",
              "merchant_id                    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY09Cu5GuNrp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}